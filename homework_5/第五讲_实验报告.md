# 第五讲作业 实验报告

舒意恒 MF20330067 计算机系

## 问题一

构造的两类数据如图所示：

![双月数据](img/5_1_data.jpg)

单层感知机训练结果：见 `perceptron_log.txt`，预测准确率 0.722

BP 神经网络训练结果：见 `neural_network_log.txt`，预测准确率 0.999

该数据是线性不可分的；单层感知机的分类能力有限，在训练过程中 loss 很快进入波动状态，预测准确率停止提升；BP 神经网络模型设定隐藏层数量为 2，隐藏层大小为 50，具有一定的非线性分类能力，预测准确率高达 99.9% 左右，能区分圆内的数据点和圆外的数据点。 

## 问题二

程序日志见 `log` 目录，包含训练集上的 loss 变化、测试集上的 loss 以及预测准确率。

在 2000 个 epoch 内，使用同一模型与不同 hidden_size，测试集上达到的最高准确率：

- hidden size 50: 0.9643
- hidden_size 100: 0.9732 
- hidden_size 150: 0.9716
- hidden_size 300: 0.9772

一定程度上，增加隐藏层大小可以增强模型的能力。

可以尝试通过增加隐藏层数量，或改用适用于图像任务的 CNN 进行建模，以进一步提高识别准确率。 


## 问题三

设定 loss 在 0.0001 时训练停止，训练停止时误差为 9.9453e-05，迭代次数 2267.
隐藏层大小被设定为 50.


输入层、输出层的权重和偏置：

```
input layer weight: Parameter containing:
tensor([[-0.9717],
        [-2.1392],
        [-0.1570],
        [ 0.7957],
        [ 1.3497],
        [ 1.3860],
        [ 0.3780],
        [-2.3568],
        [ 2.5272],
        [-0.4327],
        [ 2.6209],
        [-0.7226],
        [-1.3151],
        [-0.6990],
        [-0.9230],
        [-2.4128],
        [ 0.2310],
        [-1.9370],
        [ 1.1139],
        [-1.4864],
        [ 2.4593],
        [-0.3183],
        [ 0.9639],
        [ 2.7031],
        [ 2.4455],
        [ 2.3016],
        [-0.8501],
        [-0.7521],
        [ 0.9084],
        [ 1.5628],
        [-0.9558],
        [-0.5662],
        [-0.4910],
        [-0.2638],
        [ 0.6799],
        [-3.8330],
        [-0.3821],
        [ 1.9645],
        [-0.4853],
        [-0.1246],
        [ 1.0090],
        [-0.7043],
        [ 0.2056],
        [ 1.4823],
        [ 1.1703],
        [-0.5271],
        [ 2.0426],
        [ 1.2739],
        [-0.7504],
        [-0.1011]], requires_grad=True)

input layer bias: Parameter containing:
tensor([ 1.3077,  0.1050,  0.8194, -0.2086,  1.3966, -0.3625, -0.2679,  0.3499,
         1.3109, -0.6513,  0.2061, -1.6584,  1.3018, -1.3482,  0.9101, -1.2244,
        -0.8580, -2.2513, -0.5851, -2.0778,  0.4648, -0.6957, -0.8705,  0.5751,
         1.6017,  0.8525, -0.4415, -0.9035,  0.3206,  0.0296,  0.9628, -1.4470,
         0.7447, -1.2241, -0.2609, -3.0322,  0.3811,  0.3597,  0.4861,  1.6222,
         1.5071,  0.6857, -0.2411,  0.0873,  1.0224, -0.5086,  0.9456,  1.4058,
         0.4165,  0.6945], requires_grad=True)

output layer weight: Parameter containing:
tensor([[ 0.5210, -0.1529, -2.0080, -0.1246,  1.6778, -0.3886,  0.0911, -0.4490,
         -0.4176,  0.4998,  0.5001,  2.3392, -0.3862,  1.4899, -1.1306,  0.1459,
         -0.1770, -0.0560,  1.0735, -0.3157, -0.7337, -0.6758,  0.4303, -0.3111,
         -0.1518, -0.2947, -0.0431,  0.5054,  0.1512,  0.4697, -0.2261,  0.0627,
          2.1974,  0.0120, -0.3740, -0.0336,  0.5796,  0.0605, -1.8971, -0.1890,
          2.2867,  0.4084, -0.4259, -0.5331,  0.2595,  0.9539, -0.5967,  0.1110,
          0.5321,  0.2642]], requires_grad=True)

output layer bias: Parameter containing:
tensor([-0.0434], requires_grad=True)

```